{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN\n",
    "based on [xhujoy's implementation](https://github.com/XHUJOY/CycleGAN-tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T09:19:58.441778Z",
     "start_time": "2017-12-20T09:19:57.323848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "import os,argparse,math,time,copy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from glob import glob\n",
    "from collections import namedtuple\n",
    "from tensorflow.python.framework import ops\n",
    "from custom_ops import batch_norm,instance_norm,conv2d,deconv2d,lrelu,linear\n",
    "%matplotlib inline\n",
    "print (\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make cycle-GAN graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T09:19:58.646261Z",
     "start_time": "2017-12-20T09:19:58.443165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic module ready\n"
     ]
    }
   ],
   "source": [
    "###### Discriminator for cycle-GAN\n",
    "def discriminator(image, options, reuse=False, name=\"discriminator\"):\n",
    "    with tf.variable_scope(name):\n",
    "        # image is 256 x 256 x input_c_dim\n",
    "        if reuse: tf.get_variable_scope().reuse_variables()\n",
    "        else: assert tf.get_variable_scope().reuse is False\n",
    "        h0 = lrelu(conv2d(image,options.df_dim,_name='d_h0_conv'))\n",
    "        # h0 is (128 x 128 x self.df_dim)\n",
    "        h1 = lrelu(instance_norm(conv2d(h0, options.df_dim*2,_name='d_h1_conv'),'d_bn1'))\n",
    "        # h1 is (64 x 64 x self.df_dim*2)\n",
    "        h2 = lrelu(instance_norm(conv2d(h1, options.df_dim*4,_name='d_h2_conv'),'d_bn2'))\n",
    "        # h2 is (32x 32 x self.df_dim*4)\n",
    "        h3 = lrelu(instance_norm(conv2d(h2, options.df_dim*8,_s=1,_name='d_h3_conv'),'d_bn3'))\n",
    "        # h3 is (32 x 32 x self.df_dim*8)\n",
    "        h4 = conv2d(h3,1,_s=1,_name='d_h3_pred')\n",
    "        # h4 is (32 x 32 x 1) # Patch-GAN loss\n",
    "        return h4\n",
    "\n",
    "# U-net style generator\n",
    "def generator_unet(image, options, reuse=False, name=\"generator\"):\n",
    "\n",
    "    # Dropout with 50% off?!?!\n",
    "    dropout_rate = 0.5 if options.is_training else 1.0\n",
    "    with tf.variable_scope(name):\n",
    "        # image is 256 x 256 x input_c_dim\n",
    "        if reuse: tf.get_variable_scope().reuse_variables()\n",
    "        else: assert tf.get_variable_scope().reuse is False\n",
    "\n",
    "        # Encoder part (lrelu)\n",
    "        # image is (256 x 256 x input_c_dim)\n",
    "        e1 = instance_norm(conv2d(image,options.gf_dim,_name='g_e1_conv'))\n",
    "        # e1 is (128 x 128 x self.gf_dim)\n",
    "        e2 = instance_norm(conv2d(lrelu(e1),options.gf_dim*2,_name='g_e2_conv'),'g_bn_e2')\n",
    "        # e2 is (64 x 64 x self.gf_dim*2)\n",
    "        e3 = instance_norm(conv2d(lrelu(e2),options.gf_dim*4,_name='g_e3_conv'),'g_bn_e3')\n",
    "        # e3 is (32 x 32 x self.gf_dim*4)\n",
    "        e4 = instance_norm(conv2d(lrelu(e3),options.gf_dim*8,_name='g_e4_conv'),'g_bn_e4')\n",
    "        # e4 is (16 x 16 x self.gf_dim*8)\n",
    "        e5 = instance_norm(conv2d(lrelu(e4),options.gf_dim*8,_name='g_e5_conv'),'g_bn_e5')\n",
    "        # e5 is (8 x 8 x self.gf_dim*8)\n",
    "        e6 = instance_norm(conv2d(lrelu(e5),options.gf_dim*8,_name='g_e6_conv'),'g_bn_e6')\n",
    "        # e6 is (4 x 4 x self.gf_dim*8)\n",
    "        e7 = instance_norm(conv2d(lrelu(e6),options.gf_dim*8,_name='g_e7_conv'),'g_bn_e7')\n",
    "        # e7 is (2 x 2 x self.gf_dim*8)\n",
    "        e8 = instance_norm(conv2d(lrelu(e7),options.gf_dim*8,_name='g_e8_conv'),'g_bn_e8')\n",
    "        # e8 is (1 x 1 x self.gf_dim*8)\n",
    "\n",
    "        # Decoder part (relu+dropout)\n",
    "        d1 = deconv2d(tf.nn.relu(e8),options.gf_dim*8,_name='g_d1')\n",
    "        d1 = tf.nn.dropout(d1,dropout_rate)\n",
    "        d1 = tf.concat([instance_norm(d1,'g_bn_d1'),e7],axis=3)\n",
    "        # d1 is (2 x 2 x self.gf_dim*8*2)\n",
    "\n",
    "        d2 = deconv2d(tf.nn.relu(d1),options.gf_dim*8,_name='g_d2')\n",
    "        d2 = tf.nn.dropout(d2,dropout_rate)\n",
    "        d2 = tf.concat([instance_norm(d2,'g_bn_d2'),e6],axis=3)\n",
    "        # d2 is (4 x 4 x self.gf_dim*8*2)\n",
    "\n",
    "        d3 = deconv2d(tf.nn.relu(d2),options.gf_dim*8,_name='g_d3')\n",
    "        d3 = tf.nn.dropout(d3,dropout_rate)\n",
    "        d3 = tf.concat([instance_norm(d3,'g_bn_d3'),e5],axis=3)\n",
    "        # d3 is (8 x 8 x self.gf_dim*8*2)\n",
    "\n",
    "        d4 = deconv2d(tf.nn.relu(d3),options.gf_dim*8,_name='g_d4')\n",
    "        d4 = tf.concat([instance_norm(d4,'g_bn_d4'),e4],axis=3)\n",
    "        # d4 is (16 x 16 x self.gf_dim*8*2)\n",
    "\n",
    "        d5 = deconv2d(tf.nn.relu(d4),options.gf_dim*4,_name='g_d5')\n",
    "        d5 = tf.concat([instance_norm(d5,'g_bn_d5'),e3],axis=3)\n",
    "        # d5 is (32 x 32 x self.gf_dim*4*2)\n",
    "\n",
    "        d6 = deconv2d(tf.nn.relu(d5),options.gf_dim*2,_name='g_d6')\n",
    "        d6 = tf.concat([instance_norm(d6,'g_bn_d6'),e2],axis=3)\n",
    "        # d6 is (64 x 64 x self.gf_dim*2*2)\n",
    "\n",
    "        d7 = deconv2d(tf.nn.relu(d6),options.gf_dim,_name='g_d7')\n",
    "        d7 = tf.concat([instance_norm(d7,'g_bn_d7'),e1],axis=3)\n",
    "        # d7 is (128 x 128 x self.gf_dim*1*2)\n",
    "\n",
    "        d8 = deconv2d(tf.nn.relu(d7),options.output_c_dim,_name='g_d8')\n",
    "        # d8 is (256 x 256 x output_c_dim)\n",
    "        # Back to original image\n",
    "\n",
    "        return tf.nn.tanh(d8)\n",
    "    \n",
    "# ResNet style generator\n",
    "def generator_resnet(image, options, reuse=False, name=\"generator\"):\n",
    "    with tf.variable_scope(name):\n",
    "        # image is 256 x 256 x input_c_dim\n",
    "        if reuse: tf.get_variable_scope().reuse_variables()\n",
    "        else: assert tf.get_variable_scope().reuse is False\n",
    "\n",
    "        def residule_block(x, dim, ks=3, s=1, name='res'):\n",
    "            # amount of required padding for conv2d with 'VALID' padding \n",
    "            p = int((ks - 1) / 2)\n",
    "            y = tf.pad(x, [[0, 0], [p, p], [p, p], [0, 0]], \"REFLECT\")\n",
    "            y = instance_norm(conv2d(y, dim, ks, s,_padding='VALID',_name=name+'_c1'),name+'_bn1')\n",
    "            y = tf.pad(tf.nn.relu(y), [[0, 0], [p, p], [p, p], [0, 0]], \"REFLECT\")\n",
    "            y = instance_norm(conv2d(y, dim, ks, s,_padding='VALID',_name=name+'_c2'),name+'_bn2')\n",
    "            return y + x\n",
    "        # residual block \n",
    "\n",
    "        # Justin Johnson's model from https://github.com/jcjohnson/fast-neural-style/\n",
    "        # The network with 9 blocks consists of: c7s1-32, d64, d128, R128, R128, R128,\n",
    "        # R128, R128, R128, R128, R128, R128, u64, u32, c7s1-3\n",
    "        c0 = tf.pad(image, [[0, 0], [3, 3], [3, 3], [0, 0]], \"REFLECT\")\n",
    "        # c0 is (256+6 x 256+6 x 3)\n",
    "        c1 = tf.nn.relu(instance_norm(conv2d(c0,options.gf_dim,_ks=7,_s=1,_padding='VALID',_name='g_e1_c'),'g_e1_bn'))\n",
    "        # c1 is (256 x 256 x gf_dim)\n",
    "        c2 = tf.nn.relu(instance_norm(conv2d(c1,options.gf_dim*2,_ks=3,_s=2,_name='g_e2_c'), 'g_e2_bn'))\n",
    "        # c2 is (128 x 128 x gf_dim*2)\n",
    "        c3 = tf.nn.relu(instance_norm(conv2d(c2,options.gf_dim*4,_ks=3,_s=2,_name='g_e3_c'), 'g_e3_bn'))\n",
    "        # c3 is (64 x 64 x gf_dim*4)\n",
    "        # define G network with 9 resnet blocks\n",
    "        r1 = residule_block(c3, options.gf_dim*4, name='g_r1')\n",
    "        # r1 is (64 x 64 x gf_dim*4)\n",
    "        r2 = residule_block(r1, options.gf_dim*4, name='g_r2')\n",
    "        r3 = residule_block(r2, options.gf_dim*4, name='g_r3')\n",
    "        r4 = residule_block(r3, options.gf_dim*4, name='g_r4')\n",
    "        r5 = residule_block(r4, options.gf_dim*4, name='g_r5')\n",
    "        r6 = residule_block(r5, options.gf_dim*4, name='g_r6')\n",
    "        r7 = residule_block(r6, options.gf_dim*4, name='g_r7')\n",
    "        r8 = residule_block(r7, options.gf_dim*4, name='g_r8')\n",
    "        r9 = residule_block(r8, options.gf_dim*4, name='g_r9')\n",
    "        # r9 is (64 x 64 x gf_dim*4)\n",
    "\n",
    "        d1 = deconv2d(r9,options.gf_dim*2,_ks=3,_s=2,_name='g_d1_dc')\n",
    "        d1 = tf.nn.relu(instance_norm(d1,'g_d1_bn'))\n",
    "        # d1 is (128 x 128 x gf_dim*2)\n",
    "        d2 = deconv2d(d1, options.gf_dim,_ks=3,_s=2,_name='g_d2_dc')\n",
    "        d2 = tf.nn.relu(instance_norm(d2,'g_d2_bn'))\n",
    "        d2 = tf.pad(d2, [[0, 0], [3, 3], [3, 3], [0, 0]], \"REFLECT\")\n",
    "        # d2 is (256+6 x 256+6 x gf_dim)\n",
    "        pred = tf.nn.tanh(conv2d(d2,options.output_c_dim,_ks=7,_s=1,_padding='VALID',_name='g_pred_c'))\n",
    "        # pred is (256 x 256 x output_c_dim)\n",
    "        return pred\n",
    "    \n",
    "# Absolute difference\n",
    "def abs_criterion(in_, target):\n",
    "    return tf.reduce_mean(tf.abs(in_ - target))\n",
    "\n",
    "# Least-square loss: more like a regression loss \n",
    "def mae_criterion(in_, target):\n",
    "    return tf.reduce_mean((in_-target)**2)\n",
    "\n",
    "# Original D-loss: cross entropy loss\n",
    "def sce_criterion(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "print (\"Basic module ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T09:19:58.766586Z",
     "start_time": "2017-12-20T09:19:58.647506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Util ready\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Read image (gray or rgb) with float (0.0~255.0) \"\"\"\n",
    "def imread_float(path, is_grayscale = False):\n",
    "    if (is_grayscale):\n",
    "        return scipy.misc.imread(path, flatten=True).astype(np.float)\n",
    "    else:\n",
    "        return scipy.misc.imread(path, mode='RGB').astype(np.float)\n",
    "\n",
    "\"\"\" Center crop \"\"\"\n",
    "def center_crop(x, crop_h, crop_w,\n",
    "                resize_h=64, resize_w=64):\n",
    "    if crop_w is None:\n",
    "        crop_w = crop_h\n",
    "    h, w = x.shape[:2]\n",
    "    j = int(round((h - crop_h)/2.))\n",
    "    i = int(round((w - crop_w)/2.))\n",
    "    return scipy.misc.imresize(x[j:j+crop_h, i:i+crop_w], [resize_h, resize_w])\n",
    "\n",
    "\"\"\" Normalize pixel val from -1.0 to +1.0 \"\"\"\n",
    "def transform(image, npx=64, is_crop=True, resize_w=64):\n",
    "    # npx : # of pixels width/height of image\n",
    "    if is_crop:\n",
    "        cropped_image = center_crop(image, npx, resize_w=resize_w)\n",
    "    else:\n",
    "        cropped_image = image\n",
    "    return np.array(cropped_image)/127.5 - 1.\n",
    "\n",
    "\"\"\" Change pixel values ranging from -1.0 to +1.0 to 0.0 to 1.0 \"\"\"\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "\"\"\" Image pool \"\"\"\n",
    "class ImagePool(object):\n",
    "    def __init__(self, maxsize=50):\n",
    "        self.maxsize = maxsize\n",
    "        self.num_img = 0\n",
    "        self.images = []\n",
    "    def __call__(self, image):\n",
    "        \"\"\" Input 'image' is a list consists of two images 'fakeA' and 'fakeB' \"\"\"\n",
    "        if self.maxsize <= 0:\n",
    "            return image\n",
    "        \"\"\" Save image to the image pool as many as possible \"\"\"\n",
    "        if self.num_img < self.maxsize:\n",
    "            self.images.append(image)\n",
    "            self.num_img += 1\n",
    "            return image\n",
    "        \"\"\" If the pool if full \"\"\"\n",
    "        if np.random.rand() > 0.5:\n",
    "            \"\"\" with half prob,  \"\"\"\n",
    "            \"\"\" Randomly replace 'fakeA' and 'fakeB' into 'self.images' \"\"\"\n",
    "            idx = int(np.random.rand()*self.maxsize) # random index\n",
    "            tmp1 = copy.copy(self.images[idx])[0]\n",
    "            self.images[idx][0] = image[0]\n",
    "            idx = int(np.random.rand()*self.maxsize)\n",
    "            tmp2 = copy.copy(self.images[idx])[1]\n",
    "            self.images[idx][1] = image[1]\n",
    "            return [tmp1, tmp2]\n",
    "        else:\n",
    "            \"\"\" return full pool \"\"\"\n",
    "            return image\n",
    "\n",
    "    \n",
    "def load_test_data(image_path, fine_size=256):\n",
    "    img = imread_float(image_path)\n",
    "    img = scipy.misc.imresize(img, [fine_size, fine_size])\n",
    "    img = img/127.5 - 1\n",
    "    return img\n",
    "\n",
    "def load_train_data(image_path, load_size=286, fine_size=256, is_testing=False):\n",
    "    img_A = imread_float(image_path[0])\n",
    "    img_B = imread_floatimread(image_path[1])\n",
    "    if not is_testing:\n",
    "        # For training phase, \n",
    "        img_A = q\n",
    "        img_B = scipy.misc.imresize(img_B, [load_size, load_size])\n",
    "        # Random crop \n",
    "        h1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n",
    "        w1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n",
    "        img_A = img_A[h1:h1+fine_size, w1:w1+fine_size]\n",
    "        img_B = img_B[h1:h1+fine_size, w1:w1+fine_size]\n",
    "        # Random flip augmentation\n",
    "        if np.random.random() > 0.5:\n",
    "            img_A = np.fliplr(img_A)\n",
    "            img_B = np.fliplr(img_B)\n",
    "    else:\n",
    "        img_A = scipy.misc.imresize(img_A, [fine_size, fine_size])\n",
    "        img_B = scipy.misc.imresize(img_B, [fine_size, fine_size])\n",
    "    # Normalize pixel values from -1.0 to +1.0\n",
    "    img_A = img_A/127.5 - 1.\n",
    "    img_B = img_B/127.5 - 1.\n",
    "    # Concatenate two images from different domains A and B\n",
    "    img_AB = np.concatenate((img_A, img_B), axis=2)\n",
    "    # img_AB shape: (fine_size, fine_size, input_c_dim + output_c_dim)\n",
    "    return img_AB\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "\"\"\" Read images \"\"\"\n",
    "def get_image(image_path, image_size, is_crop=True, resize_w=64, is_grayscale = False):\n",
    "    return transform(imread_float(image_path, is_grayscale), image_size, is_crop, resize_w)\n",
    "\n",
    "\"\"\" Save images \"\"\"\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "\"\"\" Save images \"\"\"\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "# ??\n",
    "def merge_images(images, size):\n",
    "    return inverse_transform(images)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "print (\"Util ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycle-GAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T09:19:59.315522Z",
     "start_time": "2017-12-20T09:19:58.767706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyclegan class ready\n"
     ]
    }
   ],
   "source": [
    "class cyclegan(object):\n",
    "    def __init__(self, sess, args):\n",
    "        self.sess = sess\n",
    "        self.batch_size = args.batch_size\n",
    "        self.image_size = args.fine_size\n",
    "        self.input_c_dim = args.input_nc\n",
    "        self.output_c_dim = args.output_nc\n",
    "        self.L1_lambda = args.L1_lambda\n",
    "        self.dataset_dir = args.dataset_dir\n",
    "\n",
    "        self.discriminator = discriminator\n",
    "        if args.use_resnet:\n",
    "            self.generator = generator_resnet\n",
    "        else:\n",
    "            self.generator = generator_unet\n",
    "        if args.use_lsgan:\n",
    "            self.criterionGAN = mae_criterion\n",
    "        else:\n",
    "            self.criterionGAN = sce_criterion\n",
    "\n",
    "        OPTIONS = namedtuple('OPTIONS', 'batch_size image_size \\\n",
    "                              gf_dim df_dim output_c_dim is_training')\n",
    "        self.options = OPTIONS._make((args.batch_size, args.fine_size,\n",
    "                                      args.ngf, args.ndf, args.output_nc,\n",
    "                                      args.phase == 'train'))\n",
    "\n",
    "        self._build_model()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.pool = ImagePool(args.max_size)\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Real images come from both domain A and B\n",
    "        self.real_data = tf.placeholder(tf.float32,\n",
    "                                        [None, self.image_size, self.image_size,\n",
    "                                         self.input_c_dim + self.output_c_dim],\n",
    "                                        name='real_A_and_B_images')\n",
    "        # Divide input and output \n",
    "        self.real_A = self.real_data[:, :, :, :self.input_c_dim]\n",
    "        self.real_B = self.real_data[:, :, :, self.input_c_dim:self.input_c_dim + self.output_c_dim]\n",
    "        # Generator\n",
    "        self.fake_B  = self.generator(self.real_A, self.options,reuse=False, name=\"generatorA2B\")\n",
    "        self.fake_A_ = self.generator(self.fake_B, self.options,reuse=False, name=\"generatorB2A\")\n",
    "        self.fake_A  = self.generator(self.real_B, self.options,reuse=True, name=\"generatorB2A\")\n",
    "        self.fake_B_ = self.generator(self.fake_A, self.options,reuse=True, name=\"generatorA2B\")\n",
    "        # Discriminator\n",
    "        self.DA_fake = self.discriminator(self.fake_A, self.options, reuse=False, name=\"discriminatorA\")\n",
    "        self.DB_fake = self.discriminator(self.fake_B, self.options, reuse=False, name=\"discriminatorB\")\n",
    "        # G losses \n",
    "        self.g_loss_a2b = self.criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n",
    "        self.g_loss_b2a = self.criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n",
    "        # Total G loss for training (adversarial loss + consistency loss)\n",
    "        self.g_loss = self.criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) \\\n",
    "            + self.criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n",
    "            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n",
    "        # Image to convert \n",
    "        self.fake_A_sample = tf.placeholder(tf.float32,\n",
    "                                            [None, self.image_size, self.image_size,\n",
    "                                             self.input_c_dim], name='fake_A_sample')\n",
    "        self.fake_B_sample = tf.placeholder(tf.float32,\n",
    "                                            [None, self.image_size, self.image_size,\n",
    "                                             self.output_c_dim], name='fake_B_sample')\n",
    "        # D losses\n",
    "        self.DB_real = self.discriminator(self.real_B, self.options, reuse=True, name=\"discriminatorB\")\n",
    "        self.DA_real = self.discriminator(self.real_A, self.options, reuse=True, name=\"discriminatorA\")\n",
    "        self.DB_fake_sample = self.discriminator(self.fake_B_sample, self.options, reuse=True, name=\"discriminatorB\")\n",
    "        self.DA_fake_sample = self.discriminator(self.fake_A_sample, self.options, reuse=True, name=\"discriminatorA\")\n",
    "        # Real fake losses (real to one / fake to zero)\n",
    "        self.db_loss_real = self.criterionGAN(self.DB_real, tf.ones_like(self.DB_real))\n",
    "        self.db_loss_fake = self.criterionGAN(self.DB_fake_sample, tf.zeros_like(self.DB_fake_sample))\n",
    "        self.db_loss = (self.db_loss_real + self.db_loss_fake) / 2\n",
    "        self.da_loss_real = self.criterionGAN(self.DA_real, tf.ones_like(self.DA_real))\n",
    "        self.da_loss_fake = self.criterionGAN(self.DA_fake_sample, tf.zeros_like(self.DA_fake_sample))\n",
    "        self.da_loss = (self.da_loss_real + self.da_loss_fake) / 2\n",
    "        # Total D loss for training \n",
    "        self.d_loss = self.da_loss + self.db_loss\n",
    "\n",
    "        # Summaries\n",
    "        self.g_loss_a2b_sum = tf.summary.scalar(\"g_loss_a2b\", self.g_loss_a2b)\n",
    "        self.g_loss_b2a_sum = tf.summary.scalar(\"g_loss_b2a\", self.g_loss_b2a)\n",
    "        self.g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
    "        self.g_sum = tf.summary.merge([self.g_loss_a2b_sum, self.g_loss_b2a_sum, self.g_loss_sum])\n",
    "        self.db_loss_sum = tf.summary.scalar(\"db_loss\", self.db_loss)\n",
    "        self.da_loss_sum = tf.summary.scalar(\"da_loss\", self.da_loss)\n",
    "        self.d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "        self.db_loss_real_sum = tf.summary.scalar(\"db_loss_real\", self.db_loss_real)\n",
    "        self.db_loss_fake_sum = tf.summary.scalar(\"db_loss_fake\", self.db_loss_fake)\n",
    "        self.da_loss_real_sum = tf.summary.scalar(\"da_loss_real\", self.da_loss_real)\n",
    "        self.da_loss_fake_sum = tf.summary.scalar(\"da_loss_fake\", self.da_loss_fake)\n",
    "        self.d_sum = tf.summary.merge(\n",
    "            [self.da_loss_sum, self.da_loss_real_sum, self.da_loss_fake_sum,\n",
    "             self.db_loss_sum, self.db_loss_real_sum, self.db_loss_fake_sum,\n",
    "             self.d_loss_sum]\n",
    "        )\n",
    "\n",
    "        # Images to convert \n",
    "        self.test_A = tf.placeholder(tf.float32,\n",
    "                                     [None, self.image_size, self.image_size,\n",
    "                                      self.input_c_dim], name='test_A')\n",
    "        self.test_B = tf.placeholder(tf.float32,\n",
    "                                     [None, self.image_size, self.image_size,\n",
    "                                      self.output_c_dim], name='test_B')\n",
    "        self.testB = self.generator(self.test_A, self.options, True, name=\"generatorA2B\")\n",
    "        self.testA = self.generator(self.test_B, self.options, True, name=\"generatorB2A\")\n",
    "\n",
    "        # Get varialbles\n",
    "        t_vars = tf.trainable_variables()\n",
    "        self.d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "        self.g_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        for var in t_vars: print(var.name)\n",
    "\n",
    "    def train(self, args):\n",
    "        \"\"\"Train cyclegan\"\"\"\n",
    "        self.lr = tf.placeholder(tf.float32, None, name='learning_rate')\n",
    "        self.d_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n",
    "            .minimize(self.d_loss, var_list=self.d_vars)\n",
    "        self.g_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n",
    "            .minimize(self.g_loss, var_list=self.g_vars)\n",
    "\n",
    "        # Initialize variables\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self.sess.run(init_op)\n",
    "        self.writer = tf.summary.FileWriter(\"./logs\", self.sess.graph)\n",
    "\n",
    "        counter = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load if possible \n",
    "        if args.continue_train:\n",
    "            if self.load(args.checkpoint_dir):\n",
    "                print(\" [*] Load SUCCESS\")\n",
    "            else:\n",
    "                print(\" [!] Load failed...\")\n",
    "\n",
    "        # For each epoch\n",
    "        for epoch in range(args.epoch):\n",
    "            # Get image lists from both A and B \n",
    "            dataA = glob('../data/{}/*.*'.format(self.dataset_dir + '/trainA'))\n",
    "            dataB = glob('../data/{}/*.*'.format(self.dataset_dir + '/trainB'))\n",
    "            np.random.shuffle(dataA)\n",
    "            np.random.shuffle(dataB)\n",
    "            batch_idxs = min(min(len(dataA), len(dataB)), args.train_size) // self.batch_size\n",
    "            lr = args.lr if epoch < args.epoch_step else args.lr*(args.epoch-epoch)/(args.epoch-args.epoch_step)\n",
    "\n",
    "            # For each batch \n",
    "            for idx in range(0, batch_idxs):\n",
    "                # Load images \n",
    "                batch_files = list(zip(dataA[idx * self.batch_size:(idx + 1) * self.batch_size],\n",
    "                                       dataB[idx * self.batch_size:(idx + 1) * self.batch_size]))\n",
    "                batch_images = [load_train_data(batch_file, args.load_size, args.fine_size) for batch_file in batch_files]\n",
    "                batch_images = np.array(batch_images).astype(np.float32)\n",
    "\n",
    "                # Update G network and record fake outputs\n",
    "                fake_A, fake_B, _, summary_str = self.sess.run(\n",
    "                    [self.fake_A, self.fake_B, self.g_optim, self.g_sum],\n",
    "                    feed_dict={self.real_data: batch_images, self.lr: lr})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "                # Random pooling. ? \n",
    "                [fake_A, fake_B] = self.pool([fake_A, fake_B])\n",
    "\n",
    "                # Update D network\n",
    "                _, summary_str = self.sess.run(\n",
    "                    [self.d_optim, self.d_sum],\n",
    "                    feed_dict={self.real_data: batch_images,\n",
    "                               self.fake_A_sample: fake_A,\n",
    "                               self.fake_B_sample: fake_B,\n",
    "                               self.lr: lr})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                counter += 1\n",
    "                \n",
    "                \"\"\" print \"\"\"\n",
    "                if (np.mod(counter, args.print_freq) == 0) or (counter == 1):\n",
    "                    print((\"Epoch: [%2d] [%4d/%4d][%5d] time: %4.4f\" % (\n",
    "                        epoch, idx, batch_idxs,counter, time.time() - start_time)))\n",
    "\n",
    "                \"\"\" sample \"\"\"\n",
    "                if (np.mod(counter, args.sample_freq) == 0) or (counter == 1):\n",
    "                    self.sample_model(args.sample_dir, epoch, idx)\n",
    "\n",
    "                \"\"\" save Network \"\"\"\n",
    "                if (np.mod(counter, args.save_freq) == 0) or (counter == 1):\n",
    "                    self.save(args.checkpoint_dir, counter)\n",
    "\n",
    "    # Save model \n",
    "    def save(self, checkpoint_dir, step):\n",
    "        model_name = \"cyclegan.model\"\n",
    "        model_dir = \"%s_%s\" % (self.dataset_dir, self.image_size)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)\n",
    "        savename = os.path.join(checkpoint_dir, model_name)\n",
    "        print (\"Saving [%s]\" % (savename))\n",
    "        self.saver.save(self.sess,savename,global_step=step)\n",
    "\n",
    "    # Load model\n",
    "    def load(self, checkpoint_dir):\n",
    "        print(\" [*] Reading checkpoint...\")\n",
    "\n",
    "        model_dir = \"%s_%s\" % (self.dataset_dir, self.image_size)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Convert images and save to sample_dir\n",
    "    def sample_model(self, sample_dir, epoch, idx):\n",
    "        dataA = glob('../data/{}/*.*'.format(self.dataset_dir + '/testA'))\n",
    "        dataB = glob('../data/{}/*.*'.format(self.dataset_dir + '/testB'))\n",
    "        np.random.shuffle(dataA)\n",
    "        np.random.shuffle(dataB)\n",
    "        batch_files = list(zip(dataA[:self.batch_size], dataB[:self.batch_size]))\n",
    "        sample_images = [load_train_data(batch_file, is_testing=True) for batch_file in batch_files]\n",
    "        sample_images = np.array(sample_images).astype(np.float32)\n",
    "\n",
    "        fake_Aval,fake_Bval,real_Aval,real_Bval = self.sess.run(\n",
    "            [self.fake_A,self.fake_B,self.real_A,self.real_B],\n",
    "            feed_dict={self.real_data:sample_images}\n",
    "        )\n",
    "        if 0:\n",
    "            # Save fake B (converted from real A)\n",
    "            save_images(fake_Bval, [self.batch_size, 1],\n",
    "                        './{}/A{:02d}_{:04d}_2B.jpg'.format(sample_dir, epoch, idx))\n",
    "            save_images(real_Aval, [self.batch_size, 1],\n",
    "                        './{}/A{:02d}_{:04d}_ORG.jpg'.format(sample_dir, epoch, idx))\n",
    "            # Save fake A (converted from real B)\n",
    "            save_images(fake_Aval, [self.batch_size, 1],\n",
    "                        './{}/B{:02d}_{:04d}_2A.jpg'.format(sample_dir, epoch, idx))\n",
    "            save_images(real_Bval, [self.batch_size, 1],\n",
    "                        './{}/B{:02d}_{:04d}_ORG.jpg'.format(sample_dir, epoch, idx))\n",
    "        \n",
    "        if 1:\n",
    "            temp = np.zeros_like(fake_Aval)\n",
    "            temp[0],temp[1] = np.copy(real_Aval[0]),np.copy(fake_Bval[0])\n",
    "            scipy.misc.imsave('./{}/sample_{:02d}_{:04d}_A.jpg'.format(sample_dir, epoch, idx),\n",
    "                              merge(temp, [1,2]))\n",
    "            temp[0],temp[1] = np.copy(real_Bval[0]),np.copy(fake_Aval[0])\n",
    "            scipy.misc.imsave('./{}/sample_{:02d}_{:04d}_B.jpg'.format(sample_dir, epoch, idx),\n",
    "                              merge(temp, [1,2]))\n",
    "        print (\"Sampled\")\n",
    "        \n",
    "    # Test \n",
    "    def test(self, args):\n",
    "        \"\"\"Test cyclegan\"\"\"\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self.sess.run(init_op)\n",
    "        if args.which_direction == 'AtoB':\n",
    "            sample_files = glob('../data/{}/*.*'.format(self.dataset_dir + '/testA'))\n",
    "        elif args.which_direction == 'BtoA':\n",
    "            sample_files = glob('../data/{}/*.*'.format(self.dataset_dir + '/testB'))\n",
    "        else:\n",
    "            raise Exception('--which_direction must be AtoB or BtoA')\n",
    "\n",
    "        if self.load(args.checkpoint_dir):\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # write html for visual comparison\n",
    "        index_path = os.path.join(args.test_dir, '{0}_index.html'.format(args.which_direction))\n",
    "        index = open(index_path, \"w\")\n",
    "        index.write(\"<html><body><table><tr>\")\n",
    "        index.write(\"<th>name</th><th>input</th><th>output</th></tr>\")\n",
    "\n",
    "        out_var, in_var = (self.testB, self.test_A) if args.which_direction == 'AtoB' else (\n",
    "            self.testA, self.test_B)\n",
    "\n",
    "        for sample_file in sample_files:\n",
    "            print('Processing image: ' + sample_file)\n",
    "            sample_image = [load_test_data(sample_file, args.fine_size)]\n",
    "            sample_image = np.array(sample_image).astype(np.float32)\n",
    "            image_path = os.path.join(args.test_dir,\n",
    "                                      '{0}_{1}'.format(args.which_direction, os.path.basename(sample_file)))\n",
    "            fake_img = self.sess.run(out_var, feed_dict={in_var: sample_image})\n",
    "            save_images(fake_img, [1, 1], image_path)\n",
    "            index.write(\"<td>%s</td>\" % os.path.basename(image_path))\n",
    "            index.write(\"<td><img src='%s'></td>\" % (sample_file if os.path.isabs(sample_file) else (\n",
    "                '..' + os.path.sep + sample_file)))\n",
    "            index.write(\"<td><img src='%s'></td>\" % (image_path if os.path.isabs(image_path) else (\n",
    "                '..' + os.path.sep + image_path)))\n",
    "            index.write(\"</tr>\")\n",
    "        index.close()\n",
    "        \n",
    "print (\"cyclegan class ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate cycle-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-20T09:19:57.419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(L1_lambda=10.0, batch_size=2, beta1=0.5, checkpoint_dir='./net', continue_train=False, dataset_dir='../data/monet2photo', epoch=200, epoch_step=100, fine_size=256, input_nc=3, load_size=286, lr=0.0002, max_size=50, ndf=64, ngf=64, output_nc=3, phase='train', print_freq=100, sample_dir='./img/cyclegan/sample', sample_freq=1000, save_freq=5000, test_dir='./img/cyclegan/test', train_size=100000000.0, use_lsgan=True, use_resnet=True, which_direction='AtoB')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='CycleGAN')\n",
    "parser.add_argument('--dataset_dir', dest='dataset_dir', default='../data/monet2photo', help='path of the dataset')\n",
    "parser.add_argument('--checkpoint_dir', dest='checkpoint_dir', default='./net', help='models are saved here')\n",
    "parser.add_argument('--sample_dir', dest='sample_dir', default='./img/cyclegan/sample', help='sample are saved here')\n",
    "parser.add_argument('--test_dir', dest='test_dir', default='./img/cyclegan/test', help='test sample are saved here')\n",
    "parser.add_argument('--epoch', dest='epoch', type=int, default=200, help='# of epoch') \n",
    "parser.add_argument('--epoch_step', dest='epoch_step', type=int, default=100, help='# of epoch to decay lr')\n",
    "parser.add_argument('--batch_size', dest='batch_size', type=int, default=2, help='# images in batch')\n",
    "parser.add_argument('--train_size', dest='train_size', type=int, default=1e8, help='# images used to train')\n",
    "parser.add_argument('--load_size', dest='load_size', type=int, default=286, help='scale images to this size')\n",
    "parser.add_argument('--fine_size', dest='fine_size', type=int, default=256, help='then crop to this size')\n",
    "parser.add_argument('--ngf', dest='ngf', type=int, default=64, help='# of gen filters in first conv layer')\n",
    "parser.add_argument('--ndf', dest='ndf', type=int, default=64, help='# of discri filters in first conv layer')\n",
    "parser.add_argument('--input_nc', dest='input_nc', type=int, default=3, help='# of input image channels')\n",
    "parser.add_argument('--output_nc', dest='output_nc', type=int, default=3, help='# of output image channels')\n",
    "parser.add_argument('--lr', dest='lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
    "parser.add_argument('--beta1', dest='beta1', type=float, default=0.5, help='momentum term of adam')\n",
    "parser.add_argument('--which_direction', dest='which_direction', default='AtoB', help='AtoB or BtoA (for test only)')\n",
    "parser.add_argument('--phase', dest='phase', default='train', help='train or test')\n",
    "parser.add_argument('--print_freq', dest='print_freq', type=int, default=100, help='print the debug information every print_freq iterations')\n",
    "parser.add_argument('--save_freq', dest='save_freq', type=int, default=5000, help='save a model every save_freq iterations')\n",
    "parser.add_argument('--sample_freq', dest='sample_freq', type=int, default=1000, help='sample images every sample_freq iterations')\n",
    "parser.add_argument('--continue_train', dest='continue_train', type=bool, default=False, help='if continue training, load the latest model: 1: true, 0: false')\n",
    "parser.add_argument('--L1_lambda', dest='L1_lambda', type=float, default=10.0, help='weight on L1 term in objective')\n",
    "parser.add_argument('--use_resnet', dest='use_resnet', type=bool, default=True, help='generation network using reidule block')\n",
    "parser.add_argument('--use_lsgan', dest='use_lsgan', type=bool, default=True, help='gan loss defined in lsgan')\n",
    "parser.add_argument('--max_size', dest='max_size', type=int, default=50, help='max size of image pool, 0 means do not use image pool')\n",
    "args = parser.parse_args(args=[])\n",
    "print args "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T08:39:56.250402Z",
     "start_time": "2017-12-18T08:39:56.171421Z"
    }
   },
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-20T09:19:57.420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generatorA2B/g_e1_c/Conv/weights:0\n",
      "generatorA2B/g_e1_bn/scale:0\n",
      "generatorA2B/g_e1_bn/offset:0\n",
      "generatorA2B/g_e2_c/Conv/weights:0\n",
      "generatorA2B/g_e2_bn/scale:0\n",
      "generatorA2B/g_e2_bn/offset:0\n",
      "generatorA2B/g_e3_c/Conv/weights:0\n",
      "generatorA2B/g_e3_bn/scale:0\n",
      "generatorA2B/g_e3_bn/offset:0\n",
      "generatorA2B/g_r1_c1/Conv/weights:0\n",
      "generatorA2B/g_r1_bn1/scale:0\n",
      "generatorA2B/g_r1_bn1/offset:0\n",
      "generatorA2B/g_r1_c2/Conv/weights:0\n",
      "generatorA2B/g_r1_bn2/scale:0\n",
      "generatorA2B/g_r1_bn2/offset:0\n",
      "generatorA2B/g_r2_c1/Conv/weights:0\n",
      "generatorA2B/g_r2_bn1/scale:0\n",
      "generatorA2B/g_r2_bn1/offset:0\n",
      "generatorA2B/g_r2_c2/Conv/weights:0\n",
      "generatorA2B/g_r2_bn2/scale:0\n",
      "generatorA2B/g_r2_bn2/offset:0\n",
      "generatorA2B/g_r3_c1/Conv/weights:0\n",
      "generatorA2B/g_r3_bn1/scale:0\n",
      "generatorA2B/g_r3_bn1/offset:0\n",
      "generatorA2B/g_r3_c2/Conv/weights:0\n",
      "generatorA2B/g_r3_bn2/scale:0\n",
      "generatorA2B/g_r3_bn2/offset:0\n",
      "generatorA2B/g_r4_c1/Conv/weights:0\n",
      "generatorA2B/g_r4_bn1/scale:0\n",
      "generatorA2B/g_r4_bn1/offset:0\n",
      "generatorA2B/g_r4_c2/Conv/weights:0\n",
      "generatorA2B/g_r4_bn2/scale:0\n",
      "generatorA2B/g_r4_bn2/offset:0\n",
      "generatorA2B/g_r5_c1/Conv/weights:0\n",
      "generatorA2B/g_r5_bn1/scale:0\n",
      "generatorA2B/g_r5_bn1/offset:0\n",
      "generatorA2B/g_r5_c2/Conv/weights:0\n",
      "generatorA2B/g_r5_bn2/scale:0\n",
      "generatorA2B/g_r5_bn2/offset:0\n",
      "generatorA2B/g_r6_c1/Conv/weights:0\n",
      "generatorA2B/g_r6_bn1/scale:0\n",
      "generatorA2B/g_r6_bn1/offset:0\n",
      "generatorA2B/g_r6_c2/Conv/weights:0\n",
      "generatorA2B/g_r6_bn2/scale:0\n",
      "generatorA2B/g_r6_bn2/offset:0\n",
      "generatorA2B/g_r7_c1/Conv/weights:0\n",
      "generatorA2B/g_r7_bn1/scale:0\n",
      "generatorA2B/g_r7_bn1/offset:0\n",
      "generatorA2B/g_r7_c2/Conv/weights:0\n",
      "generatorA2B/g_r7_bn2/scale:0\n",
      "generatorA2B/g_r7_bn2/offset:0\n",
      "generatorA2B/g_r8_c1/Conv/weights:0\n",
      "generatorA2B/g_r8_bn1/scale:0\n",
      "generatorA2B/g_r8_bn1/offset:0\n",
      "generatorA2B/g_r8_c2/Conv/weights:0\n",
      "generatorA2B/g_r8_bn2/scale:0\n",
      "generatorA2B/g_r8_bn2/offset:0\n",
      "generatorA2B/g_r9_c1/Conv/weights:0\n",
      "generatorA2B/g_r9_bn1/scale:0\n",
      "generatorA2B/g_r9_bn1/offset:0\n",
      "generatorA2B/g_r9_c2/Conv/weights:0\n",
      "generatorA2B/g_r9_bn2/scale:0\n",
      "generatorA2B/g_r9_bn2/offset:0\n",
      "generatorA2B/g_d1_dc/Conv2d_transpose/weights:0\n",
      "generatorA2B/g_d1_bn/scale:0\n",
      "generatorA2B/g_d1_bn/offset:0\n",
      "generatorA2B/g_d2_dc/Conv2d_transpose/weights:0\n",
      "generatorA2B/g_d2_bn/scale:0\n",
      "generatorA2B/g_d2_bn/offset:0\n",
      "generatorA2B/g_pred_c/Conv/weights:0\n",
      "generatorB2A/g_e1_c/Conv/weights:0\n",
      "generatorB2A/g_e1_bn/scale:0\n",
      "generatorB2A/g_e1_bn/offset:0\n",
      "generatorB2A/g_e2_c/Conv/weights:0\n",
      "generatorB2A/g_e2_bn/scale:0\n",
      "generatorB2A/g_e2_bn/offset:0\n",
      "generatorB2A/g_e3_c/Conv/weights:0\n",
      "generatorB2A/g_e3_bn/scale:0\n",
      "generatorB2A/g_e3_bn/offset:0\n",
      "generatorB2A/g_r1_c1/Conv/weights:0\n",
      "generatorB2A/g_r1_bn1/scale:0\n",
      "generatorB2A/g_r1_bn1/offset:0\n",
      "generatorB2A/g_r1_c2/Conv/weights:0\n",
      "generatorB2A/g_r1_bn2/scale:0\n",
      "generatorB2A/g_r1_bn2/offset:0\n",
      "generatorB2A/g_r2_c1/Conv/weights:0\n",
      "generatorB2A/g_r2_bn1/scale:0\n",
      "generatorB2A/g_r2_bn1/offset:0\n",
      "generatorB2A/g_r2_c2/Conv/weights:0\n",
      "generatorB2A/g_r2_bn2/scale:0\n",
      "generatorB2A/g_r2_bn2/offset:0\n",
      "generatorB2A/g_r3_c1/Conv/weights:0\n",
      "generatorB2A/g_r3_bn1/scale:0\n",
      "generatorB2A/g_r3_bn1/offset:0\n",
      "generatorB2A/g_r3_c2/Conv/weights:0\n",
      "generatorB2A/g_r3_bn2/scale:0\n",
      "generatorB2A/g_r3_bn2/offset:0\n",
      "generatorB2A/g_r4_c1/Conv/weights:0\n",
      "generatorB2A/g_r4_bn1/scale:0\n",
      "generatorB2A/g_r4_bn1/offset:0\n",
      "generatorB2A/g_r4_c2/Conv/weights:0\n",
      "generatorB2A/g_r4_bn2/scale:0\n",
      "generatorB2A/g_r4_bn2/offset:0\n",
      "generatorB2A/g_r5_c1/Conv/weights:0\n",
      "generatorB2A/g_r5_bn1/scale:0\n",
      "generatorB2A/g_r5_bn1/offset:0\n",
      "generatorB2A/g_r5_c2/Conv/weights:0\n",
      "generatorB2A/g_r5_bn2/scale:0\n",
      "generatorB2A/g_r5_bn2/offset:0\n",
      "generatorB2A/g_r6_c1/Conv/weights:0\n",
      "generatorB2A/g_r6_bn1/scale:0\n",
      "generatorB2A/g_r6_bn1/offset:0\n",
      "generatorB2A/g_r6_c2/Conv/weights:0\n",
      "generatorB2A/g_r6_bn2/scale:0\n",
      "generatorB2A/g_r6_bn2/offset:0\n",
      "generatorB2A/g_r7_c1/Conv/weights:0\n",
      "generatorB2A/g_r7_bn1/scale:0\n",
      "generatorB2A/g_r7_bn1/offset:0\n",
      "generatorB2A/g_r7_c2/Conv/weights:0\n",
      "generatorB2A/g_r7_bn2/scale:0\n",
      "generatorB2A/g_r7_bn2/offset:0\n",
      "generatorB2A/g_r8_c1/Conv/weights:0\n",
      "generatorB2A/g_r8_bn1/scale:0\n",
      "generatorB2A/g_r8_bn1/offset:0\n",
      "generatorB2A/g_r8_c2/Conv/weights:0\n",
      "generatorB2A/g_r8_bn2/scale:0\n",
      "generatorB2A/g_r8_bn2/offset:0\n",
      "generatorB2A/g_r9_c1/Conv/weights:0\n",
      "generatorB2A/g_r9_bn1/scale:0\n",
      "generatorB2A/g_r9_bn1/offset:0\n",
      "generatorB2A/g_r9_c2/Conv/weights:0\n",
      "generatorB2A/g_r9_bn2/scale:0\n",
      "generatorB2A/g_r9_bn2/offset:0\n",
      "generatorB2A/g_d1_dc/Conv2d_transpose/weights:0\n",
      "generatorB2A/g_d1_bn/scale:0\n",
      "generatorB2A/g_d1_bn/offset:0\n",
      "generatorB2A/g_d2_dc/Conv2d_transpose/weights:0\n",
      "generatorB2A/g_d2_bn/scale:0\n",
      "generatorB2A/g_d2_bn/offset:0\n",
      "generatorB2A/g_pred_c/Conv/weights:0\n",
      "discriminatorA/d_h0_conv/Conv/weights:0\n",
      "discriminatorA/d_h1_conv/Conv/weights:0\n",
      "discriminatorA/d_bn1/scale:0\n",
      "discriminatorA/d_bn1/offset:0\n",
      "discriminatorA/d_h2_conv/Conv/weights:0\n",
      "discriminatorA/d_bn2/scale:0\n",
      "discriminatorA/d_bn2/offset:0\n",
      "discriminatorA/d_h3_conv/Conv/weights:0\n",
      "discriminatorA/d_bn3/scale:0\n",
      "discriminatorA/d_bn3/offset:0\n",
      "discriminatorA/d_h3_pred/Conv/weights:0\n",
      "discriminatorB/d_h0_conv/Conv/weights:0\n",
      "discriminatorB/d_h1_conv/Conv/weights:0\n",
      "discriminatorB/d_bn1/scale:0\n",
      "discriminatorB/d_bn1/offset:0\n",
      "discriminatorB/d_h2_conv/Conv/weights:0\n",
      "discriminatorB/d_bn2/scale:0\n",
      "discriminatorB/d_bn2/offset:0\n",
      "discriminatorB/d_h3_conv/Conv/weights:0\n",
      "discriminatorB/d_bn3/scale:0\n",
      "discriminatorB/d_bn3/offset:0\n",
      "discriminatorB/d_h3_pred/Conv/weights:0\n",
      "Epoch: [ 0] [   0/ 536][    1] time: 2.4724\n",
      "Sampled\n",
      "Saving [./net/../data/monet2photo_256/cyclegan.model]\n",
      "Epoch: [ 0] [  99/ 536][  100] time: 104.2610\n",
      "Epoch: [ 0] [ 199/ 536][  200] time: 205.6765\n",
      "Epoch: [ 0] [ 299/ 536][  300] time: 307.2235\n",
      "Epoch: [ 0] [ 399/ 536][  400] time: 408.8900\n",
      "Epoch: [ 0] [ 499/ 536][  500] time: 509.9682\n",
      "Epoch: [ 1] [  63/ 536][  600] time: 611.3285\n",
      "Epoch: [ 1] [ 163/ 536][  700] time: 712.8912\n",
      "Epoch: [ 1] [ 263/ 536][  800] time: 815.0380\n",
      "Epoch: [ 1] [ 363/ 536][  900] time: 916.5965\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Make directories if required \"\"\"\n",
    "if not os.path.exists(args.checkpoint_dir): os.makedirs(args.checkpoint_dir)\n",
    "if not os.path.exists(args.sample_dir): os.makedirs(args.sample_dir)\n",
    "if not os.path.exists(args.test_dir): os.makedirs(args.test_dir)\n",
    "\n",
    "tfconfig = tf.ConfigProto(allow_soft_placement=True)\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=tfconfig)\n",
    "\n",
    "\"\"\" open session and train \"\"\"\n",
    "model = cyclegan(sess, args)\n",
    "model.train(args) if args.phase == 'train' else model.test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
